{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUHSJ9o0np2BjgqpRW5q50",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day100.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q-Learning"
      ],
      "metadata": {
        "id": "A0AiO_VEqPOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q-Learning**\n",
        "\n",
        "Q-Learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given problem. It learns by interacting with an environment, updating a Q-table (a matrix of state-action values), and maximizing the expected cumulative reward. Q-Learning is effective in problems where the environment can be represented by discrete states and actions"
      ],
      "metadata": {
        "id": "YW8OcKJSqoqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the environment (4x4 grid)\n",
        "num_states = 16  # 4x4 grid\n",
        "num_actions = 4  # Up, Right, Down, Left\n",
        "q_table = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Define the parameters\n",
        "alpha = 0.1          # Learning rate\n",
        "gamma = 0.9          # Discount factor\n",
        "epsilon = 0.2        # Exploration rate\n",
        "num_episodes = 1000\n",
        "\n",
        "# Define a simple reward structure\n",
        "rewards = np.zeros(num_states)\n",
        "rewards[15] = 1  # Goal state with a reward\n",
        "\n",
        "# Function to determine the next state based on the action\n",
        "def get_next_state(state, action):\n",
        "    if action == 0 and state >= 4:               # Up\n",
        "        return state - 4\n",
        "    elif action == 1 and (state + 1) % 4 != 0:   # Right\n",
        "        return state + 1\n",
        "    elif action == 2 and state < 12:             # Down\n",
        "        return state + 4\n",
        "    elif action == 3 and state % 4 != 0:         # Left\n",
        "        return state - 1\n",
        "    else:\n",
        "        return state  # If action goes out of bounds, remain in the same state\n",
        "\n",
        "# Q-Learning algorithm\n",
        "for episode in range(num_episodes):\n",
        "    state = random.randint(0, num_states - 1)  # Start from a random state\n",
        "    while state != 15:  # Loop until reaching the goal state\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, num_actions - 1)  # Random action (exploration)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])           # Best known action (exploitation)\n",
        "\n",
        "        next_state = get_next_state(state, action)       # Get the resulting state\n",
        "        reward = rewards[next_state]                     # Get the reward for the next state\n",
        "        old_value = q_table[state, action]               # Current Q-value\n",
        "        next_max = np.max(q_table[next_state])           # Max Q-value for next state\n",
        "\n",
        "        # Q-Learning update rule\n",
        "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        state = next_state # Move to the next state\n",
        "\n",
        "# Display the learned Q-table\n",
        "print(\"Learned Q-Table:\")\n",
        "print(q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHLM_mKErA4a",
        "outputId": "9eeda6be-2a4f-412e-dd8b-eee51841385b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-Table:\n",
            "[[0.4020224  0.59049    0.36421062 0.38513481]\n",
            " [0.57665866 0.6561     0.52347809 0.49894848]\n",
            " [0.65059386 0.729      0.58464436 0.57671042]\n",
            " [0.72479514 0.72546503 0.81       0.65246629]\n",
            " [0.53144096 0.35525816 0.27318436 0.29910465]\n",
            " [0.59049    0.4186653  0.33124861 0.31882168]\n",
            " [0.6560995  0.49618813 0.41007105 0.40397628]\n",
            " [0.71562109 0.80356867 0.9        0.57959449]\n",
            " [0.47828442 0.15764974 0.17108197 0.20759823]\n",
            " [0.53143891 0.522374   0.19078838 0.20795275]\n",
            " [0.4983186  0.89999993 0.31319413 0.31636065]\n",
            " [0.80672235 0.89795832 1.         0.80672197]\n",
            " [0.42866037 0.13114445 0.08510469 0.07988133]\n",
            " [0.47663759 0.23396905 0.14567781 0.03832717]\n",
            " [0.80755639 0.40951    0.15587229 0.11805592]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}