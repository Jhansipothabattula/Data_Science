{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgnGKcqxo7dk9jB83Yskbp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Math Driven Mini Project Linear Regression from Scratch"
      ],
      "metadata": {
        "id": "4ispyVHKlks4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Linear Algebra, Calculus, and Statistics**\n",
        "\n",
        "- Linear Algebra\n",
        "\n",
        "  - Mathematical Model: $\\hat{y} = X\\theta$\n",
        "\n",
        "  X: Feature Matrix(with bias term)\n",
        "\n",
        "  $\\theta\\$: Paramters(Weights and bias)\n",
        "\n",
        "  $\\hat{y}\\$: Predicted values\n",
        "\n",
        "- Calculus\n",
        "\n",
        "  - Optimization of $\\theta\\$ involves minimizing of the loss function\n",
        "  \n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})^2\n",
        "$$\n",
        "\n",
        "  - Gradient of j($\\theta\\$):\n",
        "$$\n",
        "\\nabla J = \\frac{1}{m} X^T (X\\theta - y)\n",
        "$$\n",
        "\n",
        "- Staistics\n",
        "\n",
        "  - Metrics like Mean Squared Error(MSE) and R^2 are used to evaluate model perfomance"
      ],
      "metadata": {
        "id": "uTI1hgNhlulS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Gradient Descent for Paramter Optimization**\n",
        "- Gradient Descent Algorithm\n",
        "\n",
        "  - Iteratively update $\\theta$ using $\\theta$ :- $\\theta$ - $\\alpha$ . $\\nabla$J\n",
        "  \n",
        "  $\\alpha$ : Learning Rate\n",
        "\n",
        "- Key Steps\n",
        "\n",
        "  - Initialize Paramteters($\\theta$)\n",
        "\n",
        "  - Compute Gradients($\\nabla$J)\n",
        "\n",
        "  - Update Paramters using the Gradient Descent rule\n",
        "\n",
        "     "
      ],
      "metadata": {
        "id": "_HrsZqPXoA5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the Model using Statistical Metrics**\n",
        "- Mean Squared Error(MSE)\n",
        "  \n",
        "  - Measures the average squared error\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "$$\n",
        "\n",
        "- R-Squared($R^2$)\n",
        "\n",
        "  - Measures how well the Regression line explains the variance in the data\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
        "$$\n",
        "Where,\n",
        "$$\n",
        "SS_{\\text{res}} = \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "SS_{\\text{tot}} = \\sum_{i=1}^{m} (y^{(i)} - \\bar{y})^2\n",
        "$$"
      ],
      "metadata": {
        "id": "Wh2LeuXzp8mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 :- Implement the Mathematical Formula for Linear Regression**\n",
        "\n",
        "**Task 2 :- Use Gradient Descent to Optimize the Model Parameters**\n",
        "\n",
        "**Task 3 :- Calculation of Evalution Metrics**"
      ],
      "metadata": {
        "id": "PKkmcZRUrSBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Generate Synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "# Add bias term to feature matrix\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "# Initialize paramters\n",
        "theta = np.random.randn(2, 1)\n",
        "# Define learning rate and number of iterations\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "# Task 1 :- Implement the Mathematical Formula for Linear Regression\n",
        "def predict(X, theta):\n",
        "  return np.dot(X, theta)\n",
        "# Task 2 :- Use Gradient Descent to Optimize the Model Parameters\n",
        "def gradient_descent(X, y, theta, learning_rate, iteation):\n",
        "  m = len(y)\n",
        "  for i in range(iterations):\n",
        "    gradients = (1/m) * np.dot(X.T, (np.dot(X, theta) - y))\n",
        "    theta = theta - learning_rate * gradients\n",
        "  return theta\n",
        "# Task 3 :- Calculate evaluation Metrics\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "  return np.mean((y_true - y_pred) **2)\n",
        "def r_squared(y_true, y_pred):\n",
        "  ss_res = np.sum((y_true - y_pred) **2)\n",
        "  ss_tot = np.sum((y_true - np.mean(y_true)) **2)\n",
        "  return 1 - (ss_res / ss_tot)\n",
        "# Perform Gradient Descent\n",
        "theta_optimized = gradient_descent(X_b, y, theta, learning_rate, iterations)\n",
        "# Predictions and Evaluations\n",
        "y_pred = predict(X_b, theta_optimized)\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r_squared(y, y_pred)\n",
        "print(\"Optimized Parameters: \\n\", theta_optimized)\n",
        "print(\"Mean Squared Error: \\n\", mse)\n",
        "print(\"R-Squared: \\n\", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h49g3sNerNzd",
        "outputId": "32d345eb-46fb-4c60-e6e7-9e0fa2b5ec04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Parameters: \n",
            " [[3.90261498]\n",
            " [3.04601728]]\n",
            "Mean Squared Error: \n",
            " 0.8360749701177589\n",
            "R-Squared: \n",
            " 0.7608377030390272\n"
          ]
        }
      ]
    }
  ]
}