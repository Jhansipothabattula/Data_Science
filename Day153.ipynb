{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu/GMC1JKJIADJjo3iYAok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day153.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working with Tensors"
      ],
      "metadata": {
        "id": "xE43RBKT2C4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working with Tensors**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "* Tensors are the fundamental building blocks in PyTorch, much like arrays in NumPy but with additional capabilities tailored for deep learning.\n",
        "* Understanding how to work with tensors is crucial as they are used to represent inputs, outputs, weights, and everything in between in a neural network.\n",
        "* In this section, we will dive deep into the concept of tensors, exploring their structure, how to create them, and the various operations you can perform on them.\n",
        "* By the end of this section, you will have a strong grasp of how to manipulate tensors, which is essential for building and optimizing deep learning models.\n",
        "\n",
        "\n",
        "**Understanding Tensors: Rank, Shape, Datatype**\n",
        "\n",
        "Before diving into tensor creation and operations, it's important to understand the basic attributes of tensorsâ€”rank, shape, and datatype.\n",
        "\n",
        "* Rank (or Dimensionality)\n",
        "\n",
        "  - Definition: The rank of a tensor refers to the number of dimensions it has. For example, a scalar is a tensor with rank 0, a vector is rank 1, a matrix is rank 2, and so on.\n",
        "\n",
        "    -  Rank 0: Scalar (e.g., 3)\n",
        "    -  Rank 1: Vector (e.g., [1, 2, 3])\n",
        "    - Rank 2: Matrix (e.g., [[1, 2, 3], [4, 5, 6]])\n",
        "    - Higher Ranks:** Tensors can have higher ranks (3D, 4D, etc.) depending on the application (e.g., 3D tensors for RGB images, 4D tensors for batches of images).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Shape\n",
        "\n",
        "  - Definition:The shape of a tensor is a tuple that gives the size of each dimension. For example, a 2D tensor with shape (3, 4) represents a matrix with 3 rows and 4 columns.\n",
        "    - Example: A tensor with shape (2, 3, 4) would have 2 matrices of shape (3, 4) stacked together.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* Understanding Tensors: Rank, Shape, Datatype\n",
        "\n",
        "  * Datatype\n",
        "\n",
        "    - Definition: Tensors can store data of different types, such as integers, floats, or booleans. The datatype is important for precision and memory usage.\n",
        "\n",
        "      - Common Datatypes: torch.float32, torch.int64, torch.bool.\n",
        "      - Example: A tensor with dtype=torch.float32 would store 32-bit floating-point numbers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EEREF9Ac39DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Creating Tensors from Data\n",
        "\n",
        "Tensors can be created in various ways depending on the data you have and the requirements of your model. In this subsection, we will cover the most common methods to create tensors.\n",
        "\n",
        "* **Creating Tensors from Lists or Arrays**\n",
        "* **From Lists:** You can easily convert Python lists or nested lists into PyTorch tensors using the `torch.tensor()` function.\n",
        "* **Example:**\n",
        "```python\n",
        "import torch\n",
        "tensor_from_list = torch.tensor([1, 2, 3])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **From NumPy Arrays:** Tensors can also be created from NumPy arrays, which is useful if you are transitioning from a NumPy-based codebase.\n",
        "* **Example:**\n",
        "```python\n",
        "import numpy as np\n",
        "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_from_numpy = torch.tensor(numpy_array)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Creating Tensors with Specific Values**\n",
        "* **Zeros and Ones:** PyTorch provides functions to create tensors filled with zeros or ones, which are often used for initialization purposes.\n",
        "* **Example:**\n",
        "```python\n",
        "tensor_zeros = torch.zeros(3, 3)\n",
        "tensor_ones = torch.ones(2, 2)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Random Tensors:** Random tensors are commonly used for initializing weights in neural networks.\n",
        "* **Example:**\n",
        "```python\n",
        "random_tensor = torch.rand(4, 4)  # Uniform distribution between 0 and 1\n",
        "normal_tensor = torch.randn(4, 4) # Normal distribution with mean 0 and variance 1\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Specifying Datatypes and Devices**\n",
        "* **Datatype:** When creating a tensor, you can specify its datatype using the `dtype` argument.\n",
        "* **Example:**\n",
        "```python\n",
        "float_tensor = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Device (CPU/GPU):** Tensors can be created directly on a GPU if available, which is crucial for performance in deep learning tasks.\n",
        "* **Example:**\n",
        "```python\n",
        "gpu_tensor = torch.tensor([1, 2, 3], device='cuda')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Tensor Operations: Arithmetic, Indexing, Slicing\n",
        "\n",
        "Once tensors are created, various operations can be performed on them. This subsection covers the basics of arithmetic operations, indexing, and slicing, which are essential for manipulating and extracting data from tensors.\n",
        "\n",
        "* **Arithmetic Operations**\n",
        "* **Element-wise Operations:** PyTorch supports element-wise arithmetic operations such as addition, subtraction, multiplication, and division.\n",
        "* **Example:**\n",
        "```python\n",
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "tensor_b = torch.tensor([4, 5, 6])\n",
        "sum_tensor = tensor_a + tensor_b  # Element-wise addition\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Matrix Multiplication:** Matrix multiplication is a key operation in neural networks and can be performed using `torch.matmul()` or the `@` operator.\n",
        "* **Example:**\n",
        "```python\n",
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "result = torch.matmul(matrix_a, matrix_b)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Indexing**\n",
        "* **Single Element Access:** Similar to Python lists or NumPy arrays, you can access individual elements of a tensor using square brackets.\n",
        "* **Example:**\n",
        "```python\n",
        "tensor = torch.tensor([[1, 2], [3, 4]])\n",
        "element = tensor[0, 1]  # Access the element at row 0, column 1\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Slicing**\n",
        "* **Sub-tensors:** You can extract sub-tensors by specifying ranges of indices, similar to slicing in Python.\n",
        "* **Example:**\n",
        "```python\n",
        "tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "sub_tensor = tensor[:, 1:3]  # Extract columns 1 and 2 from all rows\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Advanced Indexing:** PyTorch also supports advanced indexing, such as selecting specific rows or columns using lists of indices.\n",
        "* **Example:**\n",
        "```python\n",
        "tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "selected_rows = tensor[[0, 2], :]  # Select the first and last row\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Broadcasting and In-Place Operations\n",
        "\n",
        "In this subsection, we'll explore more advanced tensor operations, including broadcasting, which allows for efficient computation with tensors of different shapes, and in-place operations, which modify tensors without creating new ones.\n",
        "\n",
        "* **Broadcasting**\n",
        "* **Concept:** Broadcasting is a technique used by PyTorch to perform operations on tensors of different shapes. The smaller tensor is \"broadcast\" across the larger tensor so that they have compatible shapes for element-wise operations.\n",
        "* **Example:**\n",
        "```python\n",
        "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "tensor_b = torch.tensor([1, 2, 3])\n",
        "result = tensor_a + tensor_b  # tensor_b is broadcasted to match the shape of tensor_a\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Rules:** Broadcasting follows specific rules regarding dimension matching. Understanding these rules is crucial for avoiding shape mismatches and ensuring efficient computation.\n",
        "\n",
        "\n",
        "* **In-Place Operations**\n",
        "* **Definition:** In-place operations modify the content of a tensor without allocating new memory, which can be more efficient but should be used cautiously to avoid unintended side effects.\n",
        "* **Notation:** In PyTorch, in-place operations are typically denoted by a trailing underscore (e.g., `add_()`).\n",
        "* **Example:**\n",
        "```python\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "tensor.add_(5)  # Adds 5 to each element in place, modifying the original tensor\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Use Cases:** In-place operations are useful in scenarios where memory efficiency is critical, but they can lead to bugs if the original tensor is needed elsewhere in its original state.\n",
        "\n"
      ],
      "metadata": {
        "id": "n-jYifpG5-N9"
      }
    }
  ]
}