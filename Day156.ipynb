{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO82+I0t5eKwbGmD38Hhi+T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day156.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preprocessing Data"
      ],
      "metadata": {
        "id": "HL2ZclGiAGC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Introduction**\n",
        "\n",
        "* Data is the fuel that powers neural networks.\n",
        "* However, raw data is often messy and unstructured, requiring careful preprocessing before it can be fed into a model.\n",
        "* PyTorch provides powerful tools for loading and preprocessing data, making it easier to manage large datasets and perform necessary transformations.\n",
        "* In this section, we will explore how to efficiently load data using `torchvision.datasets` and `torch.utils.data.Dataset`, apply common preprocessing techniques such as normalization and resizing, create custom datasets, and handle batch processing.\n",
        "* By the end of this section, you will be well-equipped to manage and preprocess data for your deep learning projects.\n",
        "\n"
      ],
      "metadata": {
        "id": "fkHVGDYzAkRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Loading Data with `torchvision.datasets` and `torch.utils.data.Dataset`\n",
        "\n",
        "One of the most important tasks in deep learning is efficiently loading and managing data. PyTorch provides two essential tools for this: `torchvision.datasets` for standard datasets and `torch.utils.data.Dataset` for custom datasets.\n",
        "\n",
        "### Loading Standard Datasets with `torchvision.datasets`\n",
        "\n",
        "* **Overview:** `torchvision.datasets` provides access to popular datasets such as MNIST, CIFAR-10, and ImageNet, which are commonly used for training and benchmarking deep learning models.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "import torchvision.datasets as datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist_train = datasets.MNIST(root='data', train=True, transform=ToTensor(), download=True)\n",
        "mnist_test = datasets.MNIST(root='data', train=False, transform=ToTensor(), download=True)\n",
        "\n",
        "```\n",
        "\n",
        "* **Transformations:** The transform argument allows you to apply preprocessing steps like converting images to tensors, normalizing pixel values, and more.\n",
        "\n",
        "### Custom Datasets with `torch.utils.data.Dataset`\n",
        "\n",
        "* **Overview:** When working with datasets that are not available in `torchvision.datasets`, you can create your own dataset class by subclassing `torch.utils.data.Dataset`.\n",
        "* **Creating a Custom Dataset:**\n",
        "* **Step 1:** Subclass `torch.utils.data.Dataset`.\n",
        "* **Step 2:** Implement the `__len__()` method to return the number of samples in the dataset.\n",
        "* **Step 3:** Implement the `__getitem__()` method to load and return a sample from the dataset at the given index.\n",
        "\n",
        "\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        return sample, label\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "## 2. Preprocessing Techniques: Normalization, Resizing, Transformations\n",
        "\n",
        "Preprocessing is a crucial step that can significantly impact the performance of your model. Common techniques include normalization, resizing, and other transformations that prepare data for training.\n",
        "\n",
        "### Normalization\n",
        "\n",
        "* **Definition:** Normalization involves scaling the input data to a specific range, typically [0, 1] or [-1, 1]. This helps in speeding up the training process and can lead to better convergence.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "from torchvision.transforms import Normalize\n",
        "\n",
        "# Normalize images with mean and std\n",
        "transform = Normalize(mean=[0.5], std=[0.5])\n",
        "\n",
        "```\n",
        "\n",
        "### Resizing\n",
        "\n",
        "* **Definition:** Resizing images to a fixed size is necessary when working with neural networks, as they often expect input images to have the same dimensions.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "from torchvision.transforms import Resize\n",
        "\n",
        "# Resize images to 28x28 pixels\n",
        "transform = Resize((28, 28))\n",
        "\n",
        "```\n",
        "\n",
        "### Transformations\n",
        "\n",
        "* **Overview:** PyTorch's `torchvision.transforms` module provides a variety of transformations that can be applied to images, including random cropping, flipping, rotation, and more.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, Resize\n",
        "\n",
        "# Compose multiple transformations\n",
        "transform = Compose([\n",
        "    Resize((28, 28)),\n",
        "    RandomHorizontalFlip(),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "## 3. Custom Datasets and Data Loaders (Implementation)\n",
        "\n",
        "### Custom Dataset Implementation\n",
        "\n",
        "* **Data Organization:** Custom datasets often involve loading data from various sources such as images, text files, or even databases. Organizing and handling these data efficiently is key.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_filenames = os.listdir(image_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        image = Image.open(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "```\n",
        "\n",
        "### Data Loaders\n",
        "\n",
        "* **Definition:** Data loaders are used to load data in batches, shuffle the data, and handle parallel data loading using multiple workers.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create a DataLoader for the custom dataset\n",
        "custom_dataset = ImageDataset(image_dir='path/to/images', transform=ToTensor())\n",
        "dataloader = DataLoader(custom_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "```\n",
        "\n",
        "* **Advantages:** Using data loaders improves the efficiency of the training process, especially when working with large datasets.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Batch Processing and Iterating Over Datasets\n",
        "\n",
        "Batch processing is essential for training neural networks efficiently, allowing models to process multiple samples at once.\n",
        "\n",
        "### Batching with DataLoaders\n",
        "\n",
        "* **Definition:** Batching involves dividing the dataset into smaller, manageable chunks called batches. This reduces the computational load and allows for more stable training.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "```\n",
        "\n",
        "### Iterating Over Batches\n",
        "\n",
        "* **Looping Through Data:** In the training loop, you typically iterate over the dataset in batches. Each iteration processes one batch of data.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "for batch in dataloader:\n",
        "    inputs, labels = batch\n",
        "    # Forward pass, backward pass, and optimization steps go here\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "ys9XE3A2BKF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Batch Processing and Iterating Over Datasets\n",
        "\n",
        "* **Performance Considerations**\n",
        "* **Data Parallelism:** If your system has multiple GPUs, you can leverage data parallelism to distribute batches across GPUs, speeding up the training process.\n",
        "* **Optimizing Data Loading:** Using multiple workers in the data loader (num_workers) can significantly reduce the time spent on loading data, as it allows for parallel data loading.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uUIVodHpJFWX"
      }
    }
  ]
}