{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkerZf5oZbP4VciDy/eiKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day157.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Validation"
      ],
      "metadata": {
        "id": "bT7YQ1AaJUzu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation and Validation**\n",
        "\n",
        "### Introduction\n",
        "\n",
        "* Evaluating and validating a model is a critical step in the machine learning process, as it ensures that the model not only performs well on the training data but also generalizes to unseen data.\n",
        "* This section covers the essential concepts and techniques needed to evaluate and validate machine learning models effectively.\n",
        "* We'll explore various evaluation metrics such as accuracy, precision, and recall, discuss common validation techniques, and provide strategies for monitoring model performance during training.\n",
        "* Additionally, we'll address how to handle overfitting and underfitting, two common challenges in model training.\n",
        "\n",
        "\n",
        "## Understanding Evaluation Metrics: Accuracy, Precision, Recall\n",
        "\n",
        "Evaluation metrics are used to quantify the performance of a model. Different metrics are suited for different types of problems, such as classification or regression.\n",
        "\n",
        "### **Accuracy**\n",
        "\n",
        "* **Definition:** Accuracy is the ratio of correctly predicted instances to the total instances. It is a commonly used metric for classification problems where the classes are balanced.\n",
        "* **Formula:**\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n",
        "\n",
        "* **Example:** If a model correctly classifies 90 out of 100 samples, the accuracy is 90%.\n",
        "* **Limitations:** Accuracy can be misleading in cases of imbalanced datasets, where one class significantly outnumbers the others.\n",
        "\n",
        "### **Precision**\n",
        "\n",
        "* **Definition:** Precision measures the accuracy of the positive predictions made by the model. It is particularly useful when the cost of false positives is high.\n",
        "* **Formula:**\n",
        "\n",
        "$$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$$\n",
        "\n",
        "* **Example:** In a spam detection system, precision would indicate how many of the emails classified as spam were actually spam.\n",
        "* **Use Case:** Precision is crucial in scenarios like medical diagnosis, where false positives can lead to unnecessary treatments.\n",
        "\n",
        "### **Recall**\n",
        "\n",
        "* **Definition:** Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly identify all positive instances. It is important when the cost of false negatives is high.\n",
        "* **Formula:**\n",
        "\n",
        "$$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$$\n",
        "\n",
        "* **Example:** In a cancer screening test, recall would indicate how many of the actual cancer cases were correctly identified.\n",
        "* **Use Case:** Recall is vital in applications like disease screening or fraud detection, where missing a positive case could have severe consequences.\n",
        "\n",
        "### **F1 Score**\n",
        "\n",
        "* **Definition:** The F1 score is the harmonic mean of precision and recall, providing a balanced measure when both metrics are important.\n",
        "* **Formula:**\n",
        "\n",
        "$$\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "* **Use Case:** The F1 score is particularly useful when dealing with imbalanced datasets, offering a single metric that balances precision and recall.\n",
        "\n"
      ],
      "metadata": {
        "id": "vs9DilqQJ0AL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Validation Techniques**\n",
        "\n",
        "### **Train-Validation Split**\n",
        "\n",
        "* **Overview:** The dataset is split into two parts: one for training the model and the other for validating it. A common split ratio is 80:20 or 70:30, depending on the size of the dataset.\n",
        "* **Process:**\n",
        "  * **Step 1:** Split the dataset into training and validation sets.\n",
        "  * **Step 2:** Train the model on the training set.\n",
        "  * **Step 3:** Evaluate the model on the validation set.\n",
        "\n",
        "\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "```\n",
        "\n",
        "* **Advantages:** Simple to implement and computationally efficient.\n",
        "* **Disadvantages:** The model's performance might be sensitive to the particular split, especially with small datasets.\n",
        "\n",
        "### **K-Fold Cross-Validation**\n",
        "\n",
        "* **Overview:** K-fold cross-validation involves dividing the dataset into K equal parts (or folds). The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times, with each fold used exactly once as the validation data.\n",
        "* **Process:**\n",
        "  * **Step 1:** Split the dataset into K folds.\n",
        "  * **Step 2:** Train the model K times, each time using a different fold as the validation set and the remaining K-1 folds as the training set.\n",
        "  * **Step 3:** Average the performance across all K runs.\n",
        "\n",
        "\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "scores = cross_val_score(model, X, y, cv=5) # 5-fold cross-validation\n",
        "\n",
        "```\n",
        "\n",
        "* **Advantages:** Provides a more reliable estimate of model performance, especially with small datasets.\n",
        "* **Disadvantages:** Computationally expensive, especially for large datasets or complex models.\n",
        "\n",
        "### **Stratified K-Fold**\n",
        "\n",
        "* **Overview:** A variation of K-fold cross-validation that preserves the percentage of samples for each class, ensuring that each fold is representative of the overall class distribution.\n",
        "* **Use Case:** Particularly useful in classification tasks with imbalanced datasets.\n",
        "\n",
        "\n",
        "## **Monitoring Model Performance During Training**\n",
        "\n",
        "It's essential to monitor the model's performance throughout the training process to detect issues such as overfitting or underfitting early.\n",
        "\n",
        "### **Loss Curves**\n",
        "\n",
        "* **Overview:** Plotting the training and validation loss as a function of epochs helps in visualizing how well the model is learning.\n",
        "* **Example:**\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "```\n",
        "\n",
        "* **Interpretation:** A steady decrease in training loss with a similar trend in validation loss indicates that the model is learning effectively. If the validation loss starts increasing while the training loss continues to decrease, it may indicate overfitting.\n",
        "\n",
        "### **Accuracy Curves**\n",
        "\n",
        "* **Overview:** Similar to loss curves, accuracy curves show how the model's accuracy changes over time for both training and validation datasets.\n",
        "\n",
        "### **Early Stopping**\n",
        "\n",
        "* **Overview:** Early stopping is a technique where training is halted once the model's performance on the validation set stops improving. This helps prevent overfitting.\n",
        "* **Example:** Implement early stopping by monitoring validation loss and stopping training if it doesn't improve for a certain number of epochs.\n",
        "\n",
        "\n",
        "\n",
        "## **Handling Overfitting and Underfitting**\n",
        "\n",
        "Overfitting and underfitting are common challenges in machine learning. **Overfitting** occurs when a model performs well on training data but poorly on unseen data, while **underfitting** occurs when the model is too simple to capture the underlying patterns in the data.\n",
        "\n",
        "### **Understanding Underfitting**\n",
        "\n",
        "* **Symptoms:** The model has high training and validation loss, indicating that it's too simple to capture the underlying patterns in the data.\n",
        "* **Solutions:**\n",
        "* **Increase Model Complexity:** Add more layers or units to the model to make it more expressive.\n",
        "* **Train Longer:** The model may require more epochs to learn the underlying patterns.\n",
        "* **Reduce Regularization:** If regularization is too strong, it might be preventing the model from learning effectively.\n",
        "\n",
        "\n",
        "\n",
        "### **Understanding Underfitting**\n",
        "\n",
        "* **Symptoms:** The model has very low training loss but high validation loss. It captures noise or irrelevant patterns in the training data.\n",
        "* **Solutions:**\n",
        "* **Regularization:** Techniques such as L1 or L2 regularization add a penalty to the loss function, discouraging overly complex models.\n",
        "* **Example:**\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "model = torch.nn.Linear(in_features=10, out_features=1)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.01) # L2 regularization\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "# Handling Overfitting and Underfitting\n",
        "\n",
        "- Understanding Underfitting\n",
        "\n",
        "  - Symptoms:The Model has high training and Validation loss, indicating that it's too simple to capture the underlying patterns in the data\n",
        "\n",
        "  - Solutions:\n",
        "\n",
        "    - Increase Model Complexity:Add more layers or units to the model to make it more expressive\n",
        "\n",
        "    - Train Longer:The Model may require more epochs to learn the underlying patterns\n",
        "\n",
        "    - Reduce regularization:If Regularization is too strong, it might be preventing the model from learning effectively\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qt0p50q-LJD3"
      }
    }
  ]
}