{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1I8KcQWKd+mituNAlWYB7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Data_Science/blob/main/Day161.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Deployment and Production"
      ],
      "metadata": {
        "id": "F1jUKeylLoGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "* Once a deep learning model is trained and evaluated, the next critical step is **deploying it into production.**\n",
        "* Deployment involves not only saving and loading models but also ensuring they can efficiently serve predictions in real-world applications.\n",
        "* This section will guide you through the process of saving and loading models in PyTorch, serializing models for deployment using **TorchScript and ONNX**, serving models through popular frameworks like **Flask, FastAPI, and AWS Lambda**, and implementing strategies for **model monitoring and versioning** in production.\n",
        "* By mastering these techniques, you'll be equipped to take your models from development to real-world deployment with confidence.\n",
        "\n",
        "## **Saving and Loading Models with torch.save() and torch.load()**\n",
        "\n",
        "### **Saving Models**\n",
        "\n",
        "* **State Dictionary:** In PyTorch, the recommended way to save a model is by saving its state dictionary, which contains the model's parameters (weights and biases).\n",
        "* **Example:**\n",
        "```python\n",
        "import torch\n",
        "torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Entire Model:** While saving the entire model is possible, it is less flexible and not recommended for most use cases, especially when dealing with dynamic computational graphs.\n",
        "* **Example:**\n",
        "```python\n",
        "torch.save(model, 'model.pth')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Loading Models**\n",
        "\n",
        "* **Loading State Dictionary:** To load a model, you need to first initialize the model architecture and then load the saved state dictionary into it.\n",
        "* **Example:**\n",
        "```python\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Loading Entire Model:** If you saved the entire model, you can load it directly, though this method is less flexible.\n",
        "* **Example:**\n",
        "```python\n",
        "model = torch.load('model.pth')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Model Serialization and Deployment with TorchScript and ONNX**\n",
        "\n",
        "### **TorchScript**\n",
        "\n",
        "* **Overview:** TorchScript is an intermediate representation of a PyTorch model that can be optimized and executed in a production environment without requiring a Python runtime.\n",
        "* **Scripting:** Convert a PyTorch model to TorchScript using scripting, which automatically converts the model:\n",
        "```python\n",
        "scripted_model = torch.jit.script(model)\n",
        "torch.jit.save(scripted_model, 'model_scripted.pt')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Tracing:** Alternatively, you can trace a model that has a fixed input size to TorchScript:\n",
        "```python\n",
        "traced_model = torch.jit.trace(model, example_input)\n",
        "torch.jit.save(traced_model, 'model_traced.pt')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Deployment:** TorchScript models can be deployed to environments like mobile devices, edge devices, or cloud servers where a full Python runtime might not be available.\n",
        "\n",
        "### **ONNX (Open Neural Network Exchange)**\n",
        "\n",
        "* **Overview:** ONNX is an open standard for representing machine learning models, allowing models trained in PyTorch to be deployed in a variety of platforms and runtimes, such as TensorRT or ONNX Runtime.\n",
        "* **Exporting to ONNX:** Convert a PyTorch model to the ONNX format:\n",
        "```python\n",
        "torch.onnx.export(model, example_input, 'model.onnx')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Deployment:** ONNX models can be deployed in environments that support ONNX, making it easier to integrate with other frameworks and tools beyond PyTorch.\n",
        "\n",
        "\n",
        "\n",
        "## **Serving PyTorch Models with Flask, FastAPI, and AWS Lambda**\n",
        "\n",
        "Serving a model involves setting up an API that can receive data, pass it to the model for prediction, and return the result. Various frameworks can help with this process.\n",
        "\n",
        "### **Serving with Flask**\n",
        "\n",
        "* **Overview:** Flask is a lightweight web framework that can be used to create a simple API for serving PyTorch models.\n",
        "* **Example:**\n",
        "```python\n",
        "from flask import Flask, request, jsonify\n",
        "import torch\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    data = request.get_json(force=True)\n",
        "    input_tensor = torch.tensor(data['input'])\n",
        "    output = model(input_tensor)\n",
        "    return jsonify(output.tolist())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Serving with FastAPI**\n",
        "\n",
        "* **Overview:** FastAPI is a modern, fast web framework that is well-suited for building APIs with automatic documentation and validation.\n",
        "* **Example:**\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "import torch\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load('model.pth'))\n",
        "model.eval()\n",
        "\n",
        "@app.post('/predict/')\n",
        "async def predict(input_data: List[float]):\n",
        "    input_tensor = torch.tensor(input_data)\n",
        "    output = model(input_tensor)\n",
        "    return output.tolist()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Serving with AWS Lambda**\n",
        "\n",
        "* **Overview:** AWS Lambda is a serverless computing service that lets you run code without provisioning servers. You can deploy a PyTorch model using Lambda to create a scalable and cost-effective model serving endpoint.\n",
        "* **Steps:**\n",
        "* Package your model and code.\n",
        "* Deploy to AWS Lambda using a tool like AWS SAM or the Serverless Framework.\n",
        "* Integrate with API Gateway to create an HTTP endpoint for serving predictions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Model Monitoring and Versioning in Production**\n",
        "\n",
        "Once deployed, models in production must be monitored for performance and managed through versioning to ensure reliability and continuous improvement.\n",
        "\n",
        "### **Model Monitoring**\n",
        "\n",
        "* **Importance:** Monitoring is crucial for detecting issues like **model drift**, where the model's performance degrades over time due to changes in data patterns.\n",
        "* **Metrics:** Track metrics such as prediction accuracy, latency, error rates, and resource utilization.\n",
        "* **Tools:** Use monitoring tools like Prometheus, Grafana, or specialized AI monitoring platforms like Seldon or Neptune.ai to keep track of these metrics.\n",
        "\n",
        "\n",
        "\n",
        "### **Model Versioning**\n",
        "\n",
        "* **Overview:** Versioning allows you to manage multiple versions of a model, enabling rollback to previous versions if needed, and A/B testing of different models.\n",
        "* **Techniques:** Use model registries and versioning tools like MLflow, DVC (Data Version Control), or AWS SageMaker Model Registry.\n",
        "* **Deployment Strategy:** Implement canary deployments or blue-green deployments to safely transition between model versions in production.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wWf8Jf3jM-zW"
      }
    }
  ]
}