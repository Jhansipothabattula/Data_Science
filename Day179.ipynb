{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUh3aDbDdDQLCQyqnYsbWG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Data_Science/blob/main/Day179.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DIstributed Training and Perfomance Optimization"
      ],
      "metadata": {
        "id": "zTZK_VSq3RmS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Introduction**\n",
        "\n",
        "* As deep learning models grow in complexity and size, the need for efficient training methods becomes increasingly important.\n",
        "* Distributed training allows you to scale your training across multiple GPUs or even multiple nodes, significantly reducing training time.\n",
        "* Additionally, techniques like gradient accumulation, mixed precision training, and various performance optimizations can help you make the most of your hardware resources.\n",
        "* This section will guide you through the essentials of distributed training using PyTorch's **DistributedDataParallel**, as well as strategies for optimizing model performance during training.\n",
        "* By the end of this section, you will be equipped with the tools and knowledge to train large models efficiently and effectively.\n",
        "\n",
        "\n",
        "## **Distributed Training with torch.nn.parallel.DistributedDataParallel**\n",
        "\n",
        "Distributed training enables you to leverage multiple GPUs or nodes to train your models faster. PyTorch's `torch.nn.parallel.DistributedDataParallel` (DDP) is the recommended way to distribute your training across multiple devices.\n",
        "\n",
        "### **Introduction to DistributedDataParallel**\n",
        "\n",
        "* **Overview:** DDP synchronizes gradients and updates model parameters across multiple processes running on different GPUs or nodes. This ensures that each GPU contributes to the training process, effectively parallelizing the workload.\n",
        "* **Example:** In a multi-GPU setup, each GPU processes a subset of the training data, and DDP ensures that all GPUs stay in sync by averaging gradients during backpropagation.\n",
        "\n",
        "\n",
        "\n",
        "### **Setting Up DDP**\n",
        "\n",
        "* **Step 1:** Initialize the process group for communication between GPUs.\n",
        "* **Example:**\n",
        "```python\n",
        "import torch.distributed as dist\n",
        "dist.init_process_group(backend='nccl', init_method='env://')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Step 2:** Wrap your model with `DistributedDataParallel`.\n",
        "* **Example:**\n",
        "```python\n",
        "model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Step 3:** Ensure that each process is assigned a specific GPU using the `local_rank` argument.\n",
        "* **Example:**\n",
        "```python\n",
        "torch.cuda.set_device(local_rank)\n",
        "model.cuda(local_rank)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Best Practices for DDP**\n",
        "\n",
        "* **DataLoader with DistributedSampler:** Use `torch.utils.data.distributed.DistributedSampler` to ensure that each GPU receives a different subset of the data, preventing overlap and ensuring efficient use of the dataset.\n",
        "* **Example:**\n",
        "```python\n",
        "train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, sampler=train_sampler)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Gradient Synchronization:** DDP automatically synchronizes gradients across all processes, so you don't need to manually handle gradient averaging.\n",
        "* **Handling Multiple Nodes:** For multi-node setups, ensure that the `init_method` in `init_process_group` is set up correctly to allow nodes to communicate with each other.\n",
        "\n",
        "\n",
        "## **Gradient Accumulation and Gradient Clipping**\n",
        "\n",
        "Training large models on limited GPU memory can be challenging. Gradient accumulation and gradient clipping are techniques that help manage memory and ensure stable training.\n",
        "\n",
        "### **Gradient Accumulation**\n",
        "\n",
        "* **Overview:** Gradient accumulation involves accumulating gradients over multiple forward passes before performing a backward pass and optimizer step. This allows you to effectively simulate a larger batch size than what your GPU memory can handle.\n",
        "* **Example:**\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "for i in range(accumulation_steps):\n",
        "    output = model(input)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward() # Accumulate gradients\n",
        "optimizer.step() # Update weights after accumulation\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **When to Use:** Use gradient accumulation when your desired batch size exceeds the available GPU memory, allowing you to train with larger effective batch sizes.\n",
        "\n",
        "### **Gradient Clipping**\n",
        "\n",
        "* **Overview:** Gradient clipping involves capping the gradients to a maximum value to prevent them from becoming too large, which can cause unstable training or gradient explosions.\n",
        "* **Example:**\n",
        "```python\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **When to Use:** Gradient clipping is particularly useful in training deep neural networks or recurrent models where gradients can grow exponentially during backpropagation.\n",
        "\n",
        "## **Mixed Precision Training with NVIDIA Apex**\n",
        "\n",
        "Mixed precision training leverages the capabilities of modern GPUs by using both 16-bit and 32-bit floating-point operations, leading to faster computation and reduced memory usage.\n",
        "\n",
        "### **Introduction to Mixed Precision**\n",
        "\n",
        "* **Overview:** Mixed precision training allows you to train models faster by using 16-bit floats (FP16) where possible, while still maintaining the precision of 32-bit floats (FP32) for critical operations. This is especially beneficial on NVIDIA GPUs that support Tensor Cores, which are optimized for FP16 operations.\n",
        "\n",
        "### **Setting Up Mixed Precision Training**\n",
        "\n",
        "* **Using NVIDIA Apex:** NVIDIA's Apex library provides tools for easy implementation of mixed precision training in PyTorch.\n",
        "* **Example:**\n",
        "```python\n",
        "from apex import amp\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Automatic Loss Scaling:** Apex automatically scales the loss to prevent underflow when using FP16, ensuring stable training.\n",
        "\n",
        "### **Best Practices**\n",
        "\n",
        "* **Choosing Optimization Level:** Apex offers different optimization levels (O0, O1, O2, O3) that balance between speed and precision. Start with O1 as it offers a good trade-off between performance and stability.\n",
        "* **Monitoring for NaNs:** Mixed precision training can sometimes lead to numerical instability. Monitor your training for NaNs and infinities, and use gradient clipping if necessary.\n",
        "\n",
        "## **Performance Optimization Techniques: Parallelism, Asynchronous Processing**\n",
        "\n",
        "Optimizing performance in PyTorch involves making the most of your hardware resources through parallelism and asynchronous processing.\n",
        "\n",
        "### **Parallelism**\n",
        "\n",
        "* **Data Parallelism:** Distributes the data across multiple GPUs, allowing each GPU to process a portion of the data in parallel.\n",
        "* **Example:**\n",
        "```python\n",
        "model = torch.nn.DataParallel(model)\n",
        "output = model(input)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Model Parallelism:** Splits the model itself across multiple GPUs, useful for very large models that don't fit into a single GPU's memory.\n",
        "* **Example:**\n",
        "```python\n",
        "part1.to('cuda:0')\n",
        "part2.to('cuda:1')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Asynchronous Processing**\n",
        "\n",
        "* **Asynchronous Data Loading:** Using multiple workers in DataLoader allows for asynchronous data loading, reducing the time your GPU spends idle.\n",
        "* **Example:**\n",
        "```python\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, num_workers=4)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Asynchronous CUDA Operations:** PyTorch operations on CUDA tensors are asynchronous by default, allowing the GPU to perform computations while the CPU prepares the next batch of data.\n",
        "\n",
        "### **Profiling and Optimizing**\n",
        "\n",
        "* **Profiler:** Use PyTorch's profiler to identify bottlenecks in your code and optimize accordingly.\n",
        "* **Example:**\n",
        "```python\n",
        "with torch.profiler.profile(activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]) as p:\n",
        "    model(input)\n",
        "print(p.key_averages().table(sort_by=\"self_cuda_time_total\"))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Memory Management:** Monitor GPU memory usage with `torch.cuda.memory_summary()` and optimize by clearing caches or using `torch.no_grad()` in inference mode to reduce memory consumption.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gfrqe0e17kFp"
      }
    }
  ]
}