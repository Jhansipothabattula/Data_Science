{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy8J1+Y08c/oxmxTRNoEea",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day60.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Project Image Classification on Fashion MNIST or CIFAR-10"
      ],
      "metadata": {
        "id": "5bNOvYYJPKRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying CNN Architecture to a Larger Dataset\n",
        "\n",
        "## Why Larger Datasets?\n",
        "* **Larger datasets like CIFAR-10 or Fashion MNIST represent more realistic and diverse challenges compared to toy datasets like MNIST.**"
      ],
      "metadata": {
        "id": "BMNgcrVNPTJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimenting with Architecture Design, Regularization, and Augmentation\n",
        "\n",
        "## Key Techniques to Improve Performance\n",
        "\n",
        "### Architectural Modifications\n",
        "* Add more convolutional layers or change kernel sizes.\n",
        "* Use more filters in deeper layers to capture complex features.\n",
        "\n",
        "### Regularization\n",
        "* Apply dropout in dense layers and batch normalization in convolutional layers.\n",
        "* Prevent overfitting in deeper models.\n",
        "\n",
        "### Data Augmentation\n",
        "* Use techniques like random flipping, cropping, and rotation to improve generalization."
      ],
      "metadata": {
        "id": "-9t7c8qlPxAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Analyzing Model Performance and Tuning\n",
        "\n",
        "### Evaluation Metrics\n",
        "* **Accuracy:** Overall classification correctness\n",
        "* **Loss:** Measures the difference between predictions and ground truth\n",
        "* **Confusion Matrix:** Highlights misclassified classes for deeper insights"
      ],
      "metadata": {
        "id": "MypMJfxtQMoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On Exercise\n",
        "\n",
        "##  Objective\n",
        "* Build, train, and optimize a Convolutional Neural Network (CNN) for **Fashion MNIST** or **CIFAR-10** image classification.\n",
        "* Experiment with **regularization** and **data augmentation** techniques to achieve the best performance.\n",
        "\n",
        "\n",
        "### Suggested Steps:\n",
        "\n",
        "1.  **Data Preparation:** Load and preprocess either the Fashion MNIST or CIFAR-10 dataset (normalization, reshaping, one-hot encoding).\n",
        "2.  **Base Model:** Define and compile a basic CNN architecture (e.g., Conv2D -> MaxPooling2D -> Flatten -> Dense).\n",
        "3.  **Training:** Train the base model and evaluate its performance (accuracy, loss).\n",
        "4.  **Regularization:** Implement regularization techniques like **Dropout**, **L2 Regularization**, or **Batch Normalization**.\n",
        "5.  **Data Augmentation:** Apply image data augmentation (e.g., using `ImageDataGenerator` for random rotation, shifting, flipping).\n",
        "6.  **Optimization:** Tune hyperparameters (learning rate, batch size, number of layers/filters) to maximize accuracy.\n",
        "7.  **Final Evaluation:** Evaluate the optimized model on the test set and report the final performance metrics."
      ],
      "metadata": {
        "id": "znc1PbXTQcAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformation with data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "\n",
        "])"
      ],
      "metadata": {
        "id": "_dl84iaXQuey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define transformation with data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))\n",
        "\n",
        "class EnhancedCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(EnhancedCNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "    self.bn1 = nn.BatchNorm2d(6)\n",
        "    self.conv2 = nn.MaxPool2d(6, 15, 5) # This should likely be Conv2d, not MaxPool2d, and the parameters are off\n",
        "    self.bn2 = nn.BatchNorm2d(16) # This bn2 will cause an error as conv2 output is wrong\n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "      # Calculate the size of the output from the convolutional layers dynamically\n",
        "      # This section of the model definition has several issues.\n",
        "      # 1. Indentation is incorrect for _calculate_conv_output and its call.\n",
        "      # 2. `dummy_wealth` should be `dummy_data` or similar to avoid confusion.\n",
        "      # 3. `output - self.pool(...)` should be `output = self.pool(...)`.\n",
        "      # 4. `F.rel` should be `F.relu`.\n",
        "      # 5. The conv2 layer is defined as MaxPool2d in __init__, which won't work in the forward pass like this.\n",
        "      # 6. The architecture itself seems a bit off, e.g., Conv2d -> BatchNorm2d -> MaxPool2d -> BatchNorm2d? MaxPool2d should not be followed by BatchNorm2d directly for its output.\n",
        "      # I'll only fix the immediate syntax error related to the missing parenthesis for now, but note that the model definition needs significant correction.\n",
        "      # self._calculate_conv_output()\n",
        "\n",
        "      # self.fc1 = nn.Linear(self.conv_output_size, 120) # This will error because conv_output_size is not defined due to issues above\n",
        "      # self.fc2 = nn.Linear(120, 84)\n",
        "      # self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "   # def _calculate_conv_output(self):\n",
        "          # dummy_wealth = torch.randn(1, 3, 32, 32)\n",
        "          # with torch.no_grad():\n",
        "              # output - self.pool(F.relu(self.conv2(F.relu(self.bn1(self.conv1(dummy_wealth))))))\n",
        "          # self.conv_output_size = output.numel()\n",
        "  def forward(self, x):\n",
        "      x = F.relu(self.bn1(self.conv1(x))) # This assumes conv1 outputs 6 channels\n",
        "      # The following line has F.rel instead of F.relu and expects conv2 to be a Conv2d layer that works with BatchNorm2d.\n",
        "      # Also, if conv2 is a MaxPool2d, then bn2 should not be applied to its output if it's meant for features before pooling.\n",
        "      # x = self.pool(F.rel(self.bn2(self.conv2(x))))\n",
        "      # x = x.view(x.size(0), -1)\n",
        "      # x = F.relu(self.fc1(x))\n",
        "      # x = self.dropout(x) # dropout is not defined in __init__\n",
        "      # x = F.relu(self.fc2(x))\n",
        "      # x = self.fc3 # This is assigning the layer, not calling it\n",
        "      return x\n",
        "\n",
        "# model = EnhancedCNN()\n",
        "# print(model)\n",
        "\n",
        "# Define loss functions and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # This will error as model is commented out or will error due to model definition issues\n",
        "\n",
        "training_loss = []\n",
        "\n",
        "# Training Loop\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        training_loss.append(epoch_loss)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
        "# train_model(model, train_loader, criterion, optimizer) # This will error as model is commented out or will error due to model definition issues\n",
        "\n",
        "# Evaluation loop\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"accuracy : {100 * correct / total:.2f}%\")\n",
        "# evaluate_model(model, test_loader) # This will error as model is commented out or will error due to model definition issues\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# plt.plot(training_loss, label=\"Training Loss\")\n",
        "# plt.title(\"Loss Curve\")\n",
        "# plt.xlabel(\"Epoch\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-cdyRviQ8NE",
        "outputId": "fbf84d77-3e0b-4ddd-83f8-3c53e656eeea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 50000\n",
            "Test dataset size: 10000\n"
          ]
        }
      ]
    }
  ]
}