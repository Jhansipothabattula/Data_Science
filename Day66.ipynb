{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbeFNVi/xEfqTQIm844MV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day66.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sequence-to-Sequence Models and Applications"
      ],
      "metadata": {
        "id": "BPkcLUrq2acV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequence-to-Sequence (Seq2Seq) Models and Their Architecture\n",
        "\n",
        "* **What Are Seq2Seq Models?**\n",
        "    * Map an input sequence to an output sequence of different lengths\n",
        "    * Widely used for tasks like language translation, text summarization, speech-to-text, and chatbots\n",
        "\n",
        "* **Architecture**\n",
        "    * **Encoder**\n",
        "        * Processes the input sequence and encodes it into a fixed-length vector (**context vector**)\n",
        "    * **Decoder**\n",
        "        * Takes the context vector as input and generates the output sequence, step by step\n"
      ],
      "metadata": {
        "id": "jYwYrMU72gZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder Framework for Seq2Seq Tasks\n",
        "\n",
        "* **How It Works**\n",
        "    * **Encoder**\n",
        "        * Sequentially processes the input sequence using RNN, LSTM, or GRU\n",
        "        * Produces a context vector representing the entire input sequence\n",
        "    * **Decoder**\n",
        "        * Initializes its hidden state with the encoder's context vector\n",
        "        * Generates the output sequence one token at a time\n",
        "        * Predicts the next token using the previously generated tokens\n"
      ],
      "metadata": {
        "id": "p8OXxEgu2zCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanism Overview\n",
        "\n",
        "* **Why Attention?**\n",
        "    * Standard Seq2Seq models compress the entire input sequence into a fixed-length vector, which can lead to information loss for long sequences.\n",
        "    * The Attention Mechanism dynamically focuses on different parts of the input sequence when generating each output token.\n",
        "* **How Attention Works**\n",
        "    * Calculates a weight (or score) for each input token based on its relevance to the current decoder state.\n",
        "    * Outputs a weighted sum of the encoder outputs, creating a context vector for each decoder step.\n"
      ],
      "metadata": {
        "id": "tIxk0tcs3G1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objective**\n",
        "- Build a basic Seq2seq model using LSTM for translation and experiment with hyperparameters"
      ],
      "metadata": {
        "id": "wOqOE6_Y3WyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import random # Added import for random module\n",
        "\n",
        "# Example English-to-French sentences\n",
        "english_sentences = [\"hello\", \"how are you\", \"good morning\", \"thank you\", \"good night\"]\n",
        "french_sentences = [\"bonjour\", \"comment ca va\", \"bon matin\", \"merci\", \"bonne nuit\"]\n",
        "\n",
        "# Vocabulary and tokenization\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    for sentence_str in sentences: # Corrected variable name\n",
        "        for word in sentence_str.split(): # Corrected logic to add words\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "english_vocab = build_vocab(english_sentences)\n",
        "french_vocab = build_vocab(french_sentences)\n",
        "\n",
        "# Tokenize and pad sentences\n",
        "def tokenize(sentences, vocab, max_len):\n",
        "    tokenized = []\n",
        "    for sentence in sentences:\n",
        "        tokens = [vocab.get(word, vocab[\"<UNK>\"]) for word in sentence.split()]\n",
        "        tokens = [vocab[\"<SOS>\"]] + tokens + [vocab[\"<EOS>\"]]\n",
        "        tokens += [vocab[\"<PAD>\"]] * (max_len - len(tokens))\n",
        "        tokenized.append(tokens)\n",
        "    return np.array(tokenized)\n",
        "max_len_eng = max([len(sentence.split()) for sentence in english_sentences]) + 2\n",
        "max_len_fr = max([len(sentence.split()) for sentence in french_sentences]) + 2\n",
        "\n",
        "english_data = tokenize(english_sentences, english_vocab, max_len_eng) # Changed max_len to max_len_eng for English data\n",
        "french_data = tokenize(french_sentences, french_vocab, max_len_fr)\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, src_data, tgt_data):\n",
        "        self.src_data = src_data\n",
        "        self.tgt_data = tgt_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.src_data[idx]), torch.tensor(self.tgt_data[idx])\n",
        "\n",
        "dataset = TranslationDataset(english_data, french_data)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedded = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.fc(output.squeeze(1))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "      super(Seq2Seq, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      self.device = device\n",
        "\n",
        "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
        "      batch_size = src.size(0)\n",
        "      tgt_len = tgt.size(1)\n",
        "      tgt_vocab_size = self.decoder.fc.out_features\n",
        "\n",
        "      outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "      hidden, cell = self.encoder(src)\n",
        "\n",
        "      input = tgt[:, 0]\n",
        "\n",
        "      for t in range(1, tgt_len):\n",
        "        output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "        outputs[:, t, :] = output\n",
        "        top1 = output.argmax(1)\n",
        "        input = tgt[:, t] if random.random() < teacher_forcing_ratio else top1\n",
        "      return outputs\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "input_dim = len(english_vocab)\n",
        "output_dim = len(french_vocab)\n",
        "embed_dim = 64\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "\n",
        "encoder = Encoder(input_dim, embed_dim, hidden_dim, num_layers)\n",
        "decoder = Decoder(output_dim, embed_dim, hidden_dim, num_layers)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = french_vocab[\"<PAD>\"])\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "      epoch_loss = 0\n",
        "      for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt)\n",
        "\n",
        "        output_dim_size = output.shape[2] # Store vocab size before reshaping\n",
        "        output = output[:, 1:].reshape(-1, output_dim_size) # Corrected reshape\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "      print(f\"Epoch: {epoch+1},/{num_epochs}, Loss: {epoch_loss/len(dataloader)}\")\n",
        "train(model, dataloader, optimizer, criterion, device)\n",
        "\n",
        "def translate_sentence(model, sentence, english_vocab, french_vocab, max_len_fr, device):\n",
        "  model.eval()\n",
        "\n",
        "  tokens = [english_vocab.get(word, english_vocab[\"<UNK>\"]) for word in sentence.split()]\n",
        "  tokens = [english_vocab[\"<SOS>\"]] + tokens + [english_vocab[\"<EOS>\"]]\n",
        "  src = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    hidden, cell = model.encoder(src)\n",
        "\n",
        "  tgt_vocab = {v:k for k, v in french_vocab.items()}\n",
        "  tgt_indices = [french_vocab[\"<SOS>\"]]\n",
        "  for _ in range(max_len_fr):\n",
        "    tgt_tensor = torch.tensor(tgt_indices[-1]).unsqueeze(0).to(device)\n",
        "    output, hidden, cell = model.decoder(tgt_tensor, hidden, cell)\n",
        "    pred = output.argmax(1).item()\n",
        "    tgt_indices.append(pred)\n",
        "    if pred == french_vocab[\"<EOS>\"]:\n",
        "      break\n",
        "\n",
        "  translated_sentence = [tgt_vocab[idx] for idx in tgt_indices[1:-1]] # Corrected variable name\n",
        "  return \" \".join(translated_sentence)\n",
        "\n",
        "# Test Translation\n",
        "sentence = \"good morning\"\n",
        "translation = translate_sentence(model, sentence, english_vocab, french_vocab, max_len_fr, device)\n",
        "print(f\"Translation Sentence:{translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca1VRBMk3uE1",
        "outputId": "7529973c-30de-4ac6-f07a-c0710c0ce7b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1,/20, Loss: 2.549546400705973\n",
            "Epoch: 2,/20, Loss: 2.484135548273722\n",
            "Epoch: 3,/20, Loss: 2.388967752456665\n",
            "Epoch: 4,/20, Loss: 2.291834831237793\n",
            "Epoch: 5,/20, Loss: 2.111245075861613\n",
            "Epoch: 6,/20, Loss: 1.9174563884735107\n",
            "Epoch: 7,/20, Loss: 1.8483433723449707\n",
            "Epoch: 8,/20, Loss: 1.6911557118097942\n",
            "Epoch: 9,/20, Loss: 1.5613115628560383\n",
            "Epoch: 10,/20, Loss: 1.3760685125986736\n",
            "Epoch: 11,/20, Loss: 1.3686209917068481\n",
            "Epoch: 12,/20, Loss: 1.1934099992116292\n",
            "Epoch: 13,/20, Loss: 1.0432816346486409\n",
            "Epoch: 14,/20, Loss: 0.8750132520993551\n",
            "Epoch: 15,/20, Loss: 0.8090203801790873\n",
            "Epoch: 16,/20, Loss: 0.7084643642107645\n",
            "Epoch: 17,/20, Loss: 0.5222056160370508\n",
            "Epoch: 18,/20, Loss: 0.4637317309776942\n",
            "Epoch: 19,/20, Loss: 0.5049937069416046\n",
            "Epoch: 20,/20, Loss: 0.3349197010199229\n",
            "Translation Sentence:bon matin\n"
          ]
        }
      ]
    }
  ]
}